
# Background on <br/> the Poisson-lognormal Family <br/> Model, inference

<div class="footer"></div>

---

<div class="footer"></div>

:::{.fragment}

### The Poisson Log Normal model

PLN [@AiH89] is a [multivariate generalized linear model]{.alert}, where

- the counts $\mathbf{Y}_i\in\mathbb{N}^p$ are the response variables
- the main effect is due to a linear combination of the covariates $\mathbf{x}_i\in\mathbb{R}^d$
- a vector of offsets $\mathbf{o}_i\in\mathbb{R}^p$ can be specified

:::

:::{.fragment}

$$
\quad \mathbf{Y}_i | \mathbf{Z}_i \sim^{\text{iid}} \mathcal{P}\left(\exp\{\mathbf{Z}_i\}\right), \qquad \mathbf{Z}_i \sim \mathcal{N}({\underbrace{\mathbf{o}_i + \mathbf{x}_i^\top\mathbf{B}}}_{{\boldsymbol\mu}_i},\boldsymbol\Sigma).
$$

Typically, $n\approx 10000s$, $p\approx 10000s$, $d \leq  10$

$$\mathbb V[Y_{ij}] > \mathbb E[Y_{ij}] $$

:::

---

# Why use PLN models?

<div class="footer">$\texttt{pip install pyPLNmodels}$</div>

::: {.panel-tabset}

### LDA

Suppose we aim to predict the cell type based on the scMARK dataset.


```{python}
#| echo: true
from sklearn.model_selection import train_test_split
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

def get_classif_error(data, y):
    data_train, data_test, y_train, y_test = train_test_split(data, y, test_size=0.33, random_state=42)
    lda = LDA()
    lda.fit(data_train, y_train)
    y_pred = lda.predict(data_test)
    return np.mean(y_pred != y_test)
```

### Classif on $\mathbf Y$

```{python}
#| echo: true
rna = load_scrna(n_samples = 1000)
print('Classif error:',get_classif_error(rna["endog"], rna["labels"]))
```

### Classif on $\mathbb E \left[\mathbf Z|\mathbf Y\right]$
```{python}
#| echo: true
from pyPLNmodels import Pln
latent_variables = Pln(rna["endog"]).fit().latent_variables
print("Classif error: ", get_classif_error(latent_variables, rna["labels"]))
```
:::

---

<div class="footer">$\texttt{pip install pyPLNmodels}$</div>

### Qualitative assessment

<img src="figures/plnpca_vs_pca_french.png" width="100%">

## Natural extensions of PLN

<div class="footer">$\texttt{pip install pyPLNmodels}$</div>

#### Various tasks of multivariate analysis

  - **Dimension Reduction**: rank constraint matrix $\boldsymbol\Sigma$. <small>[@PLNPCA]</small>
    $$\mathbf{Z}_i \sim \mathcal{N}({\boldsymbol\mu}_i, \boldsymbol\Sigma = \mathbf{C}\mathbf{C}^\top), \quad \mathbf{C} \in \mathcal{M}_{pk} \text{ with orthogonal columns}.$$

  - **Network inference**: sparsity constraint on inverse covariance. <small>[@PLNnetwork]</small>
  $$\mathbf{Z}_i \sim \mathcal{N}({\boldsymbol\mu}_i, \boldsymbol\Sigma = \boldsymbol\Omega^{-1}), \quad \|\boldsymbol\Omega \|_1 < c.$$

---

<div class="footer">$\texttt{pip install pyPLNmodels}$</div>

#### Various tasks of multivariate analysis: Classification

  - **Unsupervised**: mixture model in the latent space <small>[@PLNmodels]</small>
  $$\mathbf{Z}_i \mid c_i = k  \sim \mathcal{N}(\boldsymbol\mu_k, \boldsymbol\Sigma_k), \quad \text{for unknown memberships } c_i.$$

  - **Supervised**: maximize separation between groups with means <small>[@PLNmodels]</small>
  $$\mathbf{Z}_i \sim \mathcal{N}(\sum_k {\boldsymbol\mu}_k \mathbf{1}_{\{c_i= k\}}, \boldsymbol\Sigma), \quad \text{for known memberships } c_i.$$


---

<div class="footer">$\texttt{pip install pyPLNmodels}$</div>

#### Various tasks of multivariate analysis: Zero-inflation

  - **Zero-inflation**: Add a zero inflation latent variable <small>[@PLNmodels]</small>
    $$\mathbf{W}_i \sim \mathcal{B}(\sigma(\boldsymbol{\mu}^{0}_i)), \quad  \mathbf Y_i \sim (1 - \mathbf W_i)\odot \mathcal P(\exp(\mathbf Z_i))$$

  - **Dimension reduction** and **Zero Inflation**: Add a rank constraint on $\boldsymbol \Sigma$

---

<div class="footer">$\texttt{pip install pyPLNmodels}$</div>

#### Various tasks of multivariate analysis: autogressive models

  - **Autoregressive**: Assume dependency between individuals
  $$\mathbf{Z}_{i+1} = \Phi \mathbf Z_i  + \epsilon_i $$


# Estimation

<div class="footer">$\texttt{pip install pyPLNmodels}$</div>

## Estimation

<div class="footer">$\texttt{pip install pyPLNmodels}$</div>

Estimate $\theta = (\mathbf{B}, {\boldsymbol\Sigma})$, predict the $\mathbf{Z}_i$, while  the model marginal likelihood is

\begin{equation*}
p_\theta(\mathbf{Y}_i) = \int_{\mathbb{R}_p} \prod_{j=1}^p p_\theta(Y_{ij} | Z_{ij}) \, p_\theta(\mathbf{Z}_i) \mathrm{d}\mathbf{Z}_i
\end{equation*}

- Untractable likelihood

- EM requires to evaluate (some moments of) $p_\theta(\mathbf{Z} \,|\,  \mathbf{Y})$, but there is no close form!

- $\implies$ variational EM

## Variational Inference


<div class="footer">$\texttt{pip install pyPLNmodels}$</div>

Use a proxy $q_\psi$ of $p_\theta(\mathbf{Z}\,|\,\mathbf{Y})$ minimizing a divergence in a class $\mathcal{Q}$ (e.g, KÃ¼llback-Leibler)

\begin{equation*}
q_\psi(\mathbf{Z})^\star \in \arg\min_{q\in\mathcal{Q}} KL\left(q(\mathbf{Z}), p_{\theta}(\mathbf{Z} | \mathbf{Y})\right).
\end{equation*}

and maximize the ELBO (Evidence Lower BOund)

\begin{equation*}
J(\theta, \psi) = \mathbb{E}_{\psi} [\log p_\theta(\mathbf{Y}, \mathbf{Z})] + \mathcal{H}[q_\psi(\mathbf{Z})]
\end{equation*}

## Optimisation methods

<div class="footer">$\texttt{pip install pyPLNmodels}$</div>

- Standard approach: alternative maximization of the ELBO: VE-step and M-step.

- Brute-force approach:

    $$(\theta^{(t+1)}, \psi^{(t+1)}) = (\theta^{(t+1)}, \psi^{(t+1)}) + \eta_t \nabla_{\theta, \psi} J(\theta^{(t)}, \psi^{(t)})$$

## Estimator properties

<div class="footer">$\texttt{pip install pyPLNmodels}$</div>

- Variance is available for the regression coefficient of PLN model (only)
  $\implies$ **confidence intervals** and **hypothesis testing**.

- Estimator may be biased for some models
