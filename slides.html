<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>PLNmodels</title>
    <meta charset="utf-8" />
    <meta name="author" content="J. Chiquet, M. Mariadassou, S. Robin   INRAE - Applied Mathematics and Informatics Division   Last update 26 January, 2021" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
    <script src="libs/htmlwidgets/htmlwidgets.js"></script>
    <script src="libs/jquery/jquery.min.js"></script>
    <link href="libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
    <script src="libs/datatables-binding/datatables.js"></script>
    <link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
    <link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
    <script src="libs/dt-core/js/jquery.dataTables.min.js"></script>
    <link href="libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
    <script src="libs/crosstalk/js/crosstalk.min.js"></script>
    <link rel="stylesheet" href="pln.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# PLNmodels
## A collection of Poisson lognormal models <br/> for multivariate analysis of count data
### J. Chiquet, M. Mariadassou, S. Robin<br /><br /> <small>INRAE - Applied Mathematics and Informatics Division</small> <br /> <small>Last update 26 January, 2021</small>
### <br/><a href="https://pln-team.github.io/PLNmodels" class="uri">https://pln-team.github.io/PLNmodels</a>

---






# Reproducibility

## Package PLNmodels

Last stable release of **PLNmodels** on CRAN,  development version available on GitHub.


```r
install.packages("PLNmodels")
devtools::install_github("jchiquet/PLNmodels")
```


```r
library(PLNmodels)
packageVersion("PLNmodels")
```

```
## [1] '0.11.2.9017'
```

## Help and documentation

The [PLNmodels website](https://pln-team.github.io/PLNmodels/) contains

- the standard package documentation 
- a set of comprehensive vignettes for the top-level functions

all formatted with [**pkgdown**](https://pkgdown.r-lib.org)

---

class: inverse, center, middle

# 'Oaks powdery mildew' &lt;br/&gt; A motivating companion data set &lt;br/&gt; .small[See Jakuschkin, Fievet, Schwaller, Fort, Robin, and Vacher [Jak+16]]



---

# Generic form of data sets
  
Routinely gathered in ecology/microbiology/genomics 

### Data tables

  - .important[Abundances]: read counts of species/transcripts `\(j\)` in sample `\(i\)`
  - .important[Covariates]: value of environmental variable `\(k\)` in sample `\(i\)`
  - .important[Offsets]: sampling effort for species/transcripts `\(j\)` in sample `\(i\)`

### Need a framework to model _dependencies between counts_ 

  - understand .important[environmental effects] &lt;br/&gt;
      `\(\rightsquigarrow\)` explanatory models (multivariate regression, classification)
  - exhibit .important[patterns of diversity] &lt;br/&gt;
      `\(\rightsquigarrow\)` summarize the information (clustering, dimension reduction)
  - understand .important[between-species interactions] &lt;br /&gt;
      `\(\rightsquigarrow\)` 'network' inference (variable/covariance selection)
  - correct for technical and .important[confounding effects] &lt;br/&gt;
      `\(\rightsquigarrow\)` account for covariables and sampling effort

---

# Oaks powdery mildew data set overview

- .important[Microbial communities] sampled on the surface of `\(n = 116\)` oak leaves
- Communities sequenced and cleaned resulting in `\(p=114\)` OTUs (66 bacteria, 48 fungi).
- Study .important[effects of the pathogen] _E.Aphiltoïdes_ wrt communities

The `oaks` variable consists in a special data frame ready to play with, typical from ecological data sets (try `?oaks` to get additional details).

&lt;small&gt;

```r
data("oaks")
str(oaks, max.level = 1)
```

```
## 'data.frame':	116 obs. of  13 variables:
##  $ Abundance   : int [1:116, 1:114] 0 0 0 0 0 0 0 0 0 0 ...
##   ..- attr(*, "dimnames")=List of 2
##  $ Sample      : Factor w/ 116 levels "A1.02","A1.03",..: 1 2 3 4 5 6 7 8 9 10 ...
##  $ tree        : Factor w/ 3 levels "susceptible",..: 2 2 2 2 2 2 2 2 2 2 ...
##  $ branch      : int  1 1 1 1 1 1 1 1 1 2 ...
##  $ leafNo      : int  2 3 4 5 6 7 8 9 10 11 ...
##  $ distTObase  : num  217 187 180 159 148 ...
##  $ distTOtrunk : int  202 175 168 148 138 136 115 88 79 198 ...
##  $ distTOground: num  156 144 142 134 130 ...
##  $ pmInfection : int  1 0 0 1 1 1 1 5 1 0 ...
##  $ orientation : Factor w/ 2 levels "NE","SW": 2 2 2 2 2 2 2 2 2 2 ...
##  $ readsTOTfun : int  2488 2054 2122 2625 2469 3156 2513 2083 2117 2522 ...
##  $ readsTOTbac : int  8315 662 480 674 643 3096 1347 2780 1346 1042 ...
##  $ Offset      : int [1:116, 1:114] 8315 662 480 674 643 3096 1347 2780 1346 1042 ...
##   ..- attr(*, "dimnames")=List of 2
```
&lt;/small&gt;

---

# Covariates and offsets

Characterize the samples and the sampling, most important being

- `tree`: Tree status with respect to the pathogen (susceptible, intermediate or resistant)
- `distTOground`: Distance of the sampled leaf to the base of the ground
- `orientation`: Orientation of the branch (South-West SW or North-East NE)
- `readsTOTfun`: Total number of ITS1 reads for that leaf
- `readsTOTbac`: Total number of 16S reads for that leaf


```r
summary(oaks$tree)
```

```
##  susceptible intermediate    resistant 
##           39           38           39
```

```r
summary(oaks$distTOground)
```

```
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    63.0   130.2   178.5   173.8   216.2   272.0
```

`\(\rightsquigarrow\)` `readsTOTfun` and `readsTOTbac` are candidate for modeling sampling effort as offsets

---

# Abundance table (I)


```r
oaks$Abundance %&gt;% as_tibble() %&gt;% 
  dplyr::select(1:10) %&gt;% 
  head() %&gt;% DT::datatable(fillContainer = FALSE)
```

<div id="htmlwidget-07e9ca57e2e8cae12888" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-07e9ca57e2e8cae12888">{"x":{"filter":"none","fillContainer":false,"data":[["1","2","3","4","5","6"],[0,0,0,0,0,0],[0,0,0,0,0,0],[0,0,0,0,0,0],[6,0,2,1,4,77],[146,0,0,1,1,2],[1,1,0,1,1,20],[6,0,0,0,1,0],[6,0,0,4,0,20],[68,4,128,121,113,90],[0,1,0,1,0,57]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>b_OTU_1045<\/th>\n      <th>b_OTU_109<\/th>\n      <th>b_OTU_1093<\/th>\n      <th>b_OTU_11<\/th>\n      <th>b_OTU_112<\/th>\n      <th>b_OTU_1191<\/th>\n      <th>b_OTU_1200<\/th>\n      <th>b_OTU_123<\/th>\n      <th>b_OTU_13<\/th>\n      <th>b_OTU_1431<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":[1,2,3,4,5,6,7,8,9,10]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>

---

# Abundance table (II)


```r
log(1 + oaks$Abundance) %&gt;% 
  corrplot::corrplot(is.corr = FALSE,
    addgrid.col = NA,  tl.cex = .5,  cl.pos = "n")
```

![](slides_files/figure-html/glance Abundances-1.png)&lt;!-- --&gt;


&lt;!-- PLN MODEL --&gt;

---
class: inverse, center, middle

# Multivariate Poisson lognormal models &lt;br/&gt; .small[Statistical framework and inference]



---

# Models for multivariate count data

## If we were in a Gaussian world...

The .important[general linear model] [MKB79] would be appropriate! For each sample `\(i = 1,\dots,n\)`, 

`$$\underbrace{\mathbf{Y}_i}_{\text{abundances}} =  \underbrace{\mathbf{x}_i^\top \boldsymbol\Theta}_{\text{covariates}} + \underbrace{\mathbf{o}_i}_{\text{sampling effort}} + \boldsymbol\varepsilon_i, \quad \boldsymbol\varepsilon_i \sim \mathcal{N}(\mathbf{0}_p, \underbrace{\boldsymbol\Sigma}_{\text{between-species dependencies}})$$`

null covariance `\(\Leftrightarrow\)` independence `\(\rightsquigarrow\)` uncorrelated species/transcripts do not interact

`\(\rightsquigarrow\)` This model gives birth to Principal Component Analysis,  Discriminant Analysis, Gaussian Graphical Models, Gaussian Mixture models and many others `\(\dots\)`


## With count data...

There is no generic model for multivariate counts

  - Data transformation (log, `\(\sqrt{}\)`) : quick and dirty
  - Non-Gaussian multivariate distributions [Ino+17]: do not scale to data dimension yet
  - .important[Latent variable models]: interaction occur in a latent (unobserved) layer

---

# The Poisson Lognormal model (PLN)

The PLN model [AH89] is a .important[multivariate generalized linear model], where 

- the counts `\(\mathbf{Y}_i\)` are the response variables
- the main effect is due to a linear combination of the covariates `\(\mathbf{x}_i\)`
- a vector of offsets `\(\mathbf{o}_i\)` can be specified for each sample.

$$
\mathbf{Y}_i | \mathbf{Z}_i \sim \mathcal{P}\left(\exp\{\mathbf{Z}_i\}\right), \qquad \mathbf{Z}_i \sim \mathcal{N}({\mathbf{o}_i + \mathbf{x}_i^\top\boldsymbol\Theta},\boldsymbol\Sigma), \\
$$

.pull-left[The unkwown parameters are 
- `\(\boldsymbol\Theta\)`, the regression parameters
- `\(\boldsymbol\Sigma\)`, the variance-covariance matrix
]

.pull-right[
Stacking all individuals together, 
  - `\(\mathbf{Y}\)` is the `\(n\times p\)` matrix of counts  
  - `\(\mathbf{X}\)` is the `\(n\times d\)` matrix of design
  - `\(\mathbf{O}\)` is the `\(n\times p\)` matrix of offsets
]

### Properties: .small[.important[over-dispersion, arbitrary-signed covariances]]

- mean: `\(\mathbb{E}(Y_{ij}) =  \exp \left( o_{ij} + \mathbf{x}_i^\top {\boldsymbol\Theta}_{\cdot j} + \sigma_{jj}/2\right) &gt;  0\)`
- variance: `\(\mathbb{V}(Y_{ij}) = \mathbb{E}(Y_{ij}) + \mathbb{E}(Y_{ij})^2 \left( e^{\sigma_{jj}} - 1 \right) &gt; \mathbb{E}(Y_{ij})\)`
- covariance: `\(\mathrm{Cov}(Y_{ij}, Y_{ik}) = \mathbb{E}(Y_{ij}) \mathbb{E}(Y_{ik}) \left( e^{\sigma_{jk}} - 1 \right).\)`

---
# Geometrical view

![](slides_files/figure-html/PLN geometry-1.png)&lt;!-- --&gt;

---
# Inference: .small[latent model but intractable EM]
  
Estimate `\(\theta = (\boldsymbol\Theta, \boldsymbol\Sigma)\)`, predict the `\(\mathbf{Z}_i\)`, while  the model marginal likelihood is

`$$p_\theta(\mathbf{Y}_i) = \int_{\mathbb{R}_p} \prod_{j=1}^p p_\theta(Y_{ij} | Z_{ij}) \, p_\theta(\mathbf{Z}_i) \mathrm{d}\mathbf{Z}_i$$`

### Maximum likelihood for incomplete data model: EM

With `\(\mathcal{H}(p) = -\mathbb{E}_p(\log(p))\)` the entropy of `\(p\)`,

`$$\log p_\theta(\mathbf{Y}) = \mathbb{E}_{p_\theta(\mathbf{Z}\,|\,\mathbf{Y})} [\log p_\theta(\mathbf{Y}, \mathbf{Z})] + \mathcal{H}[p_\theta(\mathbf{Z}\,|\,\mathbf{Y})]$$` 

EM requires to evaluate (some moments of) `\(p_\theta(\mathbf{Z} \,|\,  \mathbf{Y})\)`, but there is no close form!

### Solutions

  - [AH89] resort on numerical integration; [Kar05] Monte-Carlo integration
  - Several heuristics, not always well motivated, found in the literature...
  - .important[Variational approach] [WJ08]: use a proxy of `\(p_\theta(\mathbf{Z}\,|\,\mathbf{Y})\)`.

---
# Variational approximation

.important[See the outstanding Stéphane Robin's Lecture]

## Principle

  - Find a proxy of the conditional distribution `\(p(\mathbf{Z}\,|\,\mathbf{Y})\)`:

`$$q(\mathbf{Z}) \approx p_\theta(\mathbf{Z} | \mathbf{Y}).$$`
  - Choose a convenient class of distribution `\(\mathcal{Q}\)` and minimize a divergence

`$$q(\mathbf{Z})^\star  \arg\min_{q\in\mathcal{Q}} D\left(q(\mathbf{Z}), p(\mathbf{Z} | \mathbf{Y})\right).$$`

## Popular choice

The Küllback-Leibler divergence .small[(error averaged wrt the approximated distribution)]

`$$KL\left(q(\mathbf{Z}), p(\mathbf{Z} | \mathbf{Y})\right) = \mathbb{E}_q\left[\log \frac{q(z)}{p(z)}\right] = \int_{\mathcal{Z}} q(z) \log \frac{q(z)}{p(z)} \mathrm{d}z.$$`

---
# Variational EM &amp; PLN

## Class of distribution: diagonal multivariate Gaussian

`$$\mathcal{Q} = \Big\{q: \quad q(\mathbf{Z}) = \prod_i q_i(\mathbf{Z}_i), \quad q_i(\mathbf{Z}_i) = \mathcal{N}\left(\mathbf{Z}_i; \mathbf{m}_i, \mathrm{diag}(\mathbf{s}_i \circ \mathbf{s}_i)\right), \mathbf{m}_i, \mathbf{s}_i\in\mathbb{R}_p \Big\}$$`

Maximize the ELBO (Evidence Lower BOund):

`$$J(\theta, q) = \log p_\theta(\mathbf{Y}) - KL[q_\theta (\mathbf{Z}) ||  p_\theta(\mathbf{Z} | \mathbf{Y})]  = \mathbb{E}_{q} [\log p_\theta(\mathbf{Y}, \mathbf{Z})] + \mathcal{H}[q(\mathbf{Z})]$$`

## Variational EM

  - VE step: find the optimal `\(q\)` (here, `\(\{(\mathbf{m}_i, \mathbf{s}_i)\}_{i=1,\dots,n} = \{\mathbf{M}, \mathbf{S}\}\)`): 
`$$q^h = \arg \max J(\theta^h, q) = \arg\min_{q \in \mathcal{Q}} KL[q(\mathbf{Z}) \,||\, p_{\theta^h}(\mathbf{Z}\,|\,\mathbf{Y})]$$`
  - M step: update `\(\hat{\theta}^h\)`
`$$\theta^h = \arg\max J(\theta, q^h) = \arg\max_{\theta} \mathbb{E}_{q} [\log p_{\theta}(\mathbf{Y}, \mathbf{Z})]$$`

---
# ELBO and gradients for PLN

Let `\(\mathbf{A} = \exp\left(\mathbf{O} + \mathbf{M} + \frac12 \mathbf{S}^2\right)\)`

### Variational bound

`$$\begin{array}{ll}
J(\mathbf{Y}) &amp; = \mathbf{1}_n^\intercal \left( \left[ \mathbf{Y} \circ (\mathbf{O} + \mathbf{M}) - \mathbf{A} + \log(\mathbf{S})\right]\right) \mathbf{1}_{p} + \frac{n}2\log|{\boldsymbol\Omega}| \\
&amp; - \frac12 \mathrm{trace}\left({\boldsymbol\Omega} \bigg[\left(\mathbf{M} - \mathbf{X}{\boldsymbol\Theta}\right)^\intercal \left(\mathbf{M} - \mathbf{X}{\boldsymbol\Theta}\right) + \mathrm{diag}(\mathbf{1}_n^\intercal\mathbf{S}^2)\bigg]\right) + \text{cst.}\\
\end{array}$$`

### M-step

`$$\hat{{\boldsymbol\Theta}} = \left(\mathbf{X}^\top \mathbf{X}\right)^{-1} \mathbf{X} \mathbf{M}, \quad 
   \hat{{\boldsymbol\Sigma}} = \frac{1}{n} \left(\mathbf{M}-\mathbf{X}\hat{{\boldsymbol\Theta}}\right)^\top \left(\mathbf{M}-\mathbf{X}\hat{\boldsymbol\Theta}\right) + \frac{1}{n} \mathrm{diag}(\mathbf{1}^\intercal\mathbf{S}^2)$$`

### Variational E-step

`$$\frac{\partial J(q)}{\partial \mathbf{M}} =  \left(\mathbf{Y} - \mathbf{A} - (\mathbf{M} - \mathbf{X}{\boldsymbol\Theta}) \mathbf{\Omega}\right), \qquad \frac{\partial J(q)}{\partial \mathbf{S}} = \frac{1}{\mathbf{S}} - \mathbf{S} \circ \mathbf{A} - \mathbf{S} \mathrm{D}_{\boldsymbol\Omega} .$$`


---

# .small[Application to the optimization of PLN models]
  
## Property of PLN variational approximation

The ELBO `\(J(\theta, q)\)` is bi-concave, i.e.
  - concave wrt `\(q = (\mathbf{M}, \mathbf{S})\)` for given `\(\theta\)` 
  - convace wrt `\(\theta = (\boldsymbol\Sigma, \boldsymbol\Theta)\)` for given `\(q\)` 
but .important[not jointly concave] in general.

## Optimization

Gradient ascent for the complete set of parameters&lt;sup&gt;1&lt;/sup&gt; `\((\mathbf{M}, \mathbf{S}, \boldsymbol\Sigma, \boldsymbol\Theta)\)`

  - **algorithm**: conservative convex separable approximations Svanberg [Sva02] &lt;br/&gt;
  - **implementation**: `NLopt` nonlinear-optimization library Johnson [Joh11] &lt;br/&gt;
  - **initialization**: LM after log-trasnformation applied independently on each variables + concatenation of the regression coefficients + Pearson residuals

.footnote[[1] Alternating between variational and model parameters is useless here &lt;br/&gt;
          [2] Optimizing on `\(\mathbf{S}\)` such as `\(\mathbf{S} \circ \mathbf{S} = \mathbf{S}^2\)` is the variance avoids positive constraints ]

---
# Properties of the variational estimators

## Classical M-estimation 

We can derive asymptotic behavior (like Hessian-based Fisher information matrix for the variance) ...

... But VEM stationary point is .important[not a log-likelihood stationary point], see [WM15]

&lt;br /&gt;

## Numerical study

- number of samples `\(n \in \{50, 100, 500, 1000, 10000\}\)`
- number of species/genes `\(p \in \{20, 200\}\)`
- number of covariates `\(d \in \{2, 5, 10\}\)`
- sampling effort `\(TSS \in \{\text{low}, \text{medium}, \text{high}\}\)` ($\approx 10^4$, `\(10^5\)` and `\(10^6\)`)
- noise level `\(\sigma^2 \in \{0.2, 0.5, 1, 2\}\)`
- `\(\boldsymbol\Sigma\)` as `\(\sigma_{jk} = \sigma^2 \rho^{|j-k|}\)`, with `\(\rho = 0.2\)`
- `\(\mathbf{\Theta}\)` with entries sampled from `\(\mathcal{N}(0,1/d)\)`

---

# Bias of `\(\hat{\boldsymbol\Theta}\)`

![Bias](slides_files/PLN_bias.png)

`\(\rightsquigarrow\)` .important[asymptotically unbiased]

---

# Root mean square error of `\(\hat{\boldsymbol\Theta}\)`

![RMSE](slides_files/PLN_rmse.png)

`\(\rightsquigarrow\)` .important[asymptotically unbiased]

---

# 95% confident interval - Coverage

![Coverage](slides_files/PLN_confint.png)

.important[variance underestimated], no trusted confidence intervals can be derived out-of-the box

---

class: inverse, center, middle

# Multivariate Poisson Regression &lt;br /&gt; .small[Illustrating the oaks powdery mildew data set]



---

# The PLN function

The `PLN` function works in the same way as `lm`: 


```r
PLN(formula = , # mandatory
    data    = , # highly recommended
    subset    , # optional  
    weights   , # optional 
    control     # optional, mostly for advanced users
    )
```

- `data` specifies where to look for the variables
- `formula` specifies the relationship between variables in `data`

  ( `\(\rightsquigarrow\)` _It builds matrices_ `\(\mathbf{Y},\mathbf{X},\mathbf{O}\)`)

- `subset` is used for subsampling the observations, it should be a .important[full length] boolean vector, not a vector of indices / sample names
- `weights` is used to weighting the observations, 
- `control` is (mainly) used for tuning the optimization and should typically not be changed.
---

# Simple PLN models on oaks data

The simplest model we can imagine only has an intercept term:


```r
M00_oaks &lt;- PLN(Abundance ~ 1, oaks)
```

`M00_oaks` is a particular `R` object with class `PLNfit` that comes with a couple methods, helpfully listed when you print the object :


```r
M00_oaks
```

```
## A multivariate Poisson Lognormal fit with full covariance model.
## ==================================================================
##  nb_param    loglik       BIC       ICL
##      6669 -50893.08 -66743.93 -70854.94
## ==================================================================
## * Useful fields
##     $model_par, $latent, $var_par, $optim_par
##     $loglik, $BIC, $ICL, $loglik_vec, $nb_param, $criteria
## * Useful S3 methods
##     print(), coef(), sigma(), vcov(), fitted(), predict(), standard_error()
```

---

# Accessing parameters


```r
coef(M00_oaks) %&gt;% head() %&gt;% t() %&gt;% knitr::kable(format = "html")
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; b_OTU_1045 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; b_OTU_109 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; b_OTU_1093 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; b_OTU_11 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; b_OTU_112 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; b_OTU_1191 &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.197219 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.089349 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -2.229673 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.729653 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.4558729 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.4939436 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

.pull-left[

```r
sigma(M00_oaks) %&gt;% 
  corrplot(is.corr=FALSE, tl.cex = .5)
```

![](slides_files/figure-html/simple PLN covariance-1.png)&lt;!-- --&gt;
]

.pull-right[

```r
sigma(M00_oaks) %&gt;% cov2cor() %&gt;% 
  corrplot(tl.cex = .5)
```

![](slides_files/figure-html/simple PLN correlation-1.png)&lt;!-- --&gt;
]

---
#  Adding Offsets and covariates

## Offset: .small[modeling sampling effort]

The predefined offset uses the total sum of reads, accounting for technologies specific to fungi and bacteria:


```r
M01_oaks &lt;- PLN(Abundance ~ 1 + offset(log(Offset)) , oaks)
```

## Covariates: .small[tree and orientation effects ('ANOVA'-like) ]

The `tree` status is a natural candidate for explaining a part of the variance.

- We chose to describe the tree effect in the regression coefficient (mean)
- A possibly spurious effect regarding the interactions  between species (covariance).


```r
M11_oaks &lt;- PLN(Abundance ~ 0 + tree + offset(log(Offset)), oaks)
```

What about adding more covariates in the model, e.g. the orientation?


```r
M21_oaks &lt;- PLN(Abundance ~  0 + tree + orientation + offset(log(Offset)), oaks)
```

---
#  Adding Offsets and covariates (II)

There is a clear gain in introducing the tree covariate in the model:


```r
rbind(M00 = M00_oaks$criteria, M01 = M01_oaks$criteria,
      M11 = M11_oaks$criteria, M21 = M21_oaks$criteria) %&gt;% 
  knitr::kable(format = "html")
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; nb_param &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; loglik &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; BIC &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; ICL &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; M00 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6669 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -50893.08 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -66743.93 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -70854.94 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; M01 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6669 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -50833.95 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -66684.80 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -70802.40 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; M11 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6897 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -50126.83 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -66519.58 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -70228.53 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; M21 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7011 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -50056.90 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -66720.61 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -70334.93 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

Looking at the coefficients `\(\mathbf{\Theta}\)` associated with `tree` bring additional insights:

![](slides_files/figure-html/oaks matrix plot-1.png)&lt;!-- --&gt;

---

# Residuals correlations

For models with offsets and tree effect in the mean:

.pull-left[

```r
sigma(M01_oaks) %&gt;% cov2cor() %&gt;% 
  corrplot(is.corr=FALSE, tl.cex = .5)
```

![](slides_files/figure-html/simple PLN covariance M01-1.png)&lt;!-- --&gt;
]

.pull-right[

```r
sigma(M11_oaks) %&gt;% cov2cor() %&gt;% 
  corrplot(tl.cex = .5)
```

![](slides_files/figure-html/simple PLN correlation M11-1.png)&lt;!-- --&gt;
]

---

# .small[Natural extensions towards multivariate analysis]

  - .important[Classification]: maximize separation between groups with means 

  `$$\mathbf{Z}_i \sim \mathcal{N}({\boldsymbol\mu}_k \mathbf{1}_{\{i\in k\}}, \boldsymbol\Sigma), \quad \text{for known memberships}.$$`
  - .important[Clustering]: mixture model in the latent space 

  `$$\mathbf{Z}_i \mid i \in k  \sim \mathcal{N}(\boldsymbol\mu_k, \boldsymbol\Sigma_k), \quad \text{for unknown memberships}.$$`
  
  - .important[Dimension Reduction]: rank constraint matrix `\(\boldsymbol\Sigma\)`.
    
    `$$\mathbf{Z}_i \sim \mathcal{N}(\boldsymbol\mu, \boldsymbol\Sigma = \mathbf{B}\mathbf{B}^\top), \quad \mathbf{B} \in \mathcal{M}_{pk} \text{ with orthogonal columns}.$$`
    
  - .important[Network inference]: sparsity constraint on inverse covariance.
  
  `$$\mathbf{Z}_i \sim \mathcal{N}(\boldsymbol\mu, \boldsymbol\Sigma = \boldsymbol\Omega^{-1}), \quad \|\boldsymbol\Omega \|_1 &lt; c.$$`
  
  `\(\rightsquigarrow\)` A variant of the variational algorithm is required for each model


---

class: inverse, center, middle

# Classification for counts &lt;br /&gt; PLN discriminant analysis



---

# Poisson Linear Discriminant Analysis

 - a PLN model with a discrete known structure with `\(K\)` groups
 - group specific main effect `\({\boldsymbol\mu}_k \in\mathbb{R}^p\)`, covariance matrix `\(\boldsymbol{\Sigma}\)` is shared among groups

`$$\begin{array}{rcl}
  \text{latent space } &amp;   \mathbf{Z}_i \sim \mathcal{N}(\mathbf{o}_i + \mathbf{x}_i^\top\boldsymbol\Theta + {\boldsymbol\mu}_k \mathbf{1}_{\{i\in k\}},\boldsymbol\Sigma) \\
  \text{observation space } &amp;  \mathbf{Y}_i | \mathbf{Z}_i \sim \mathcal{P}\left(\exp\{\mathbf{Z}_i\}\right).
\end{array}$$`
### Goal of LDA

Find the linear combinations `\(\mathbf{Z}\mathbf{v}, \mathbf{v}\in\mathbb{R}^p\)` maximizing separation between groups

&lt;img src="slides_files/figure-html/PLN_geom_lda_no_offset-1.png" style="display: block; margin: auto;" /&gt;

---
# Poisson LDA: mimick the Gaussian case

## Solution

1. Adjust a "standard" PLN with `\(\mathbf{X} \rightarrow (\mathbf{X}, \mathbf{1}_{\{i\in k\}})\)`,  `\(\boldsymbol\Theta \rightarrow (\boldsymbol\Theta, \mathbf{U} = ({\boldsymbol\mu}_1, \dots, {\boldsymbol\mu}_K))\)`

2. Use estimate `\(\boldsymbol\Sigma, \mathbf{U}\)` and `\(\boldsymbol\Theta\)` and `\(\tilde{\mathbf{Z}}_i\)` to compute
`$$\hat{\boldsymbol\Sigma}_{\text{between}} = \frac1{K-1} \sum_k n_k (\hat{\boldsymbol\mu}_k - \hat{\boldsymbol\mu}_\bullet) (\hat{\boldsymbol\mu}_k - \hat{\boldsymbol\mu}_\bullet)^\intercal$$`

3. Compute first `\(K-1\)` eigenvectors of `\(\hat{\boldsymbol\Sigma}^{-1} \hat{\boldsymbol\Sigma}_{\text{between}} = \mathbf{P}^\top \Lambda \mathbf{P}\)` (discriminant axes)

- **Graphical representation**:  
  - Center the estimated latent position `\(\tilde{Z} = \mathbb{E}_q [\mathbf{Z}] - \mathbf{o}_i - \mathbf{x}_i^\top {\boldsymbol\Theta}\)`
  - Represent `\(\tilde{Z}^\text{LDA} = \tilde{Z} \mathbf{P} \Lambda^{1/2}\)` the coordinates along the discriminant axes

- **Prediction**: For each group, 
  - Compute (variational) likelihood `\(p_k = P(\mathbf{Y}_{\text{new}} | \hat{\boldsymbol\Sigma}, \hat{\boldsymbol\Theta}, \hat{\boldsymbol\mu}_k)\)`
  - Assign `\(i\)` to group with highest posterior probability `\(\pi_k \propto \frac{n_k p_k}{n}\)`
  
---

# LDA of the oaks data set

Use the `tree` variable for grouping (`grouping` is a factor of group to be considered)


```r
myLDA_tree &lt;- 
  PLNLDA(Abundance ~ 1 + offset(log(Offset)), grouping = tree, data = oaks)
```

no need for model selection!

&lt;small&gt;

```
## Linear Discriminant Analysis for Poisson Lognormal distribution
## ==================================================================
##  nb_param    loglik      BIC       ICL
##       570 -50126.83 -51481.6 -55190.54
## ==================================================================
## * Useful fields
##     $model_par, $latent, $var_par, $optim_par
##     $loglik, $BIC, $ICL, $loglik_vec, $nb_param, $criteria
## * Useful S3 methods
##     print(), coef(), sigma(), vcov(), fitted(), predict(), standard_error()
## * Additional fields for LDA
##     $percent_var, $corr_map, $scores, $group_means
## * Additional S3 methods for LDA
##     plot.PLNLDAfit(), predict.PLNLDAfit()
```
&lt;/small&gt;

---
# LDA for oaks: covariance model

PLN-LDA can account for various model of the covariance (spherical, diagonal, full)


```r
LDA_oaks1 &lt;- PLNLDA(Abundance ~ 1 + offset(log(Offset)), 
  data = oaks, grouping = tree, control = list(covariance = "full"))

LDA_oaks2 &lt;- PLNLDA(Abundance ~ 1 + offset(log(Offset)), 
  data = oaks, grouping = tree, control = list(covariance = "diagonal"))
```

.pull-left[
![](slides_files/figure-html/plot oaks1-1.png)&lt;!-- --&gt;
]

.pull-right[
![](slides_files/figure-html/plot oaks2-1.png)&lt;!-- --&gt;
]

---
# LDA for oaks: prediction

If abundance data for new data are avalaible, their group can be predicted using the `predict` function. We illustrate this on our sample&lt;sup&gt;1&lt;/sup&gt; and compare predicted seasons to actual ones


```r
predictions &lt;- predict(LDA_oaks1, newdata = oaks, type = "response")
table(predictions, oaks$tree)
```

```
##               
## predictions    susceptible intermediate resistant
##   susceptible           39            0         0
##   intermediate           0           38         0
##   resistant              0            0        39
```

.footnote[
[1] Predicting the tree of samples used to train the model is bad practice in general and prone to overfitting, we do it for illustrative purposes only. 
]

---

class: inverse, center, middle

# Clustering for counts &lt;br /&gt; .small[PLN mixture model]



---

# Multivariate Poisson Mixture models

  - a PLN model with an additional layer assuming that the latent observations are drawn from a mixture of `\(K\)` multivariate Gaussian components. 
  
  - Each component `\(k\)` have a prior probability `\(p(i \in k) = \pi_k\)` such that `\(\sum_k \pi_k = 1\)`. 
  
  Let `\(C_i\in \{1,\dots,K\}\)`  be the multinomial variable describing the component of `\(i\)`, then
  
`$$\begin{array}{rrcl}
    \text{latent layer 1:} &amp; C_i &amp; \sim &amp; \mathcal{M}(1,\pi = (\pi_1,\dots,\pi_K)), \\
    \text{latent layer 2:} &amp;   \mathbf{Z}_i \mid C_i =  k &amp; \sim &amp; 
    \mathcal{N}({\bar{\boldsymbol\mu}}_k + \mathbf{o}_i + \mathbf{x}_i^\top\boldsymbol\Theta, \boldsymbol\Sigma_k),  \quad k = 1,\dots,K. \\
  \text{observation space } &amp;  \mathbf{Y}_i | \mathbf{Z}_i &amp; \sim &amp; \mathcal{P}\left(\exp\{\mathbf{Z}_i\}\right).
    \end{array}$$`

The unkwown parameters are 
- `\(\boldsymbol\Theta\)`, the matrix of regression parameters
- `\({\boldsymbol\mu}_k\)`, the vector of means in group `\(k\)`
- `\({\boldsymbol\Sigma}_k\)`, the variance-covariance matrix in group `\(k\)`
- `\({\boldsymbol\pi}\)` the vector of mixture proportion
- `\(C_i\)`, the group membership -- or cluster -- of sample `\(i\)`

---
# PLN mixture: additional details

## Parametrization of the covariance matrices

When `\(p\)` is large, general forms of `\({\boldsymbol\Sigma}^{(k)}\)` lead to a prohibitive number of parameters

`\(\rightsquigarrow\)` We include constrained parametrizations of the covariance matrices to reduce the computational burden and avoid over-fitting:

`$$\begin{array}{rrcll}
    \text{no restriction:} &amp; \boldsymbol\Sigma_k &amp; = &amp; \text{symmetric} &amp; \text{($ K p (p+1)/2$ parameters),} \\
    \text{diagonal covariances:} &amp; \boldsymbol\Sigma_k &amp; = &amp;\mathrm{diag}({d}_k) &amp; \text{($2 K p$ parameters),} \\
    \text{spherical covariances:} &amp; \boldsymbol\Sigma_k &amp; = &amp;  \sigma_k^2 {I} &amp; \text{($K (p + 1)$ parameters).} \\
\end{array}$$`

## Features

  - **Optimization**: _"Similar"_ variational framework (with different gradients)
  - **Model selection**: variational BIC/ICL, need restart and smoothing
      - `\(\tilde{\text{BIC}}_K = J(\theta, K) - \frac12 \log(n) \mathrm{\#param}\)`
      - `\(\tilde{\text{ICL}}_K = \tilde{\text{BIC}}_K - \mathcal{H}_{\mathcal{N}}(K) - \mathcal{H}_{\mathcal{M}}(K)\)` (two layers)

---
# Clustering of the oaks samples


```r
PLN_mixtures &lt;- 
  PLNmixture(Abundance ~ 1 + offset(log(Offset)), data = oaks,
             clusters = 1:5, control_main = list(cores = 10))
```

The ouput is of class (`PLNmixturefamily`). It a collection of `R` objects:


```r
PLN_mixtures
```

```
## --------------------------------------------------------
## COLLECTION OF 5 POISSON LOGNORMAL MODELS
## --------------------------------------------------------
##  Task: Mixture Model 
## ========================================================
##  - Number of clusters considered: from 1 to 5 
##  - Best model (regarding BIC): cluster = 4 
##  - Best model (regarding ICL): cluster = 4
```

`PLNmixturesfamily` has three methods: `plot`, `getModel`, `getBestModel`&lt;sup&gt;1&lt;/sup&gt; 

.footnote[[1] Additional help can be found with `?PLNmixturefamily`, `?getBestModel.PLNmixturefamily`, `?plot.PLNmixturefamily`]


---
# Clustering analysis: model selection (I)

The plot function gives you hints about the "right" rank/subspace size of your data


```r
plot(PLN_mixtures, criteria = c("loglik", "ICL", "BIC"))
```

![](slides_files/figure-html/plot PLNmixtures offset-1.png)&lt;!-- --&gt;

---
# Clustering analysis: model selection (II)

To extract a particular model from the collection, use `getBestModel`:


```r
myPLN_mix &lt;- getBestModel(PLN_mixtures, "BIC")
```

The extracted object has class `PLNmixturefit`. It contains various information plus `\(K\)` components with class `PLNfit`.

&lt;small&gt;

```r
myPLN_mix
```

```
## Poisson Lognormal mixture model with 4 components.
## * check fields $posteriorProb, $memberships, $mixtureParam and $components
## * check methods $plot_clustering_data, $plot_clustering_pca
## * each $component[[i]] is a PLNfit with associated methods and fields
```


```r
myPLN_mix$components[[1]]
```

```
## A multivariate Poisson Lognormal fit with spherical covariance model.
## ==================================================================
##  nb_param    loglik      BIC       ICL
##       115 -18399.77 -18673.1 -20775.65
## ==================================================================
## * Useful fields
##     $model_par, $latent, $var_par, $optim_par
##     $loglik, $BIC, $ICL, $loglik_vec, $nb_param, $criteria
## * Useful S3 methods
##     print(), coef(), sigma(), vcov(), fitted(), predict(), standard_error()
```
&lt;/small&gt;

---
# Clustering analysis: model exploration and vizualisation

.pull-left[

```r
myPLN_mix$plot_clustering_pca()
```

![](slides_files/figure-html/PLN clustering 1-1.png)&lt;!-- --&gt;
]

.pull-right[

```r
myPLN_mix$plot_clustering_data()
```

![](slides_files/figure-html/PLN clustering 2-1.png)&lt;!-- --&gt;
]

---
# Clustering analysis: validation?

&lt;img src="slides_files/figure-html/PLN clustering ARI-1.png" style="display: block; margin: auto;" /&gt;

&lt;small&gt;

```r
table(myPLN_mix$memberships, oaks$tree)
```

```
##    
##     susceptible intermediate resistant
##   1          33            0         0
##   2           6            1         3
##   3           0            0        36
##   4           0           37         0
```
&lt;/small&gt;


---

class: inverse, center, middle

# Dimension reduction and vizualisation for counts &lt;br/&gt; .small[See Chiquet, Mariadassou, and Robin [CMR18]]



---
# Poisson Lognormal model for PCA

The PLN-PCA [CMR18] model implemented in *PLNmodels* can viewed as a PLN model with an additional rank constraint on the covariance matrix `\(\boldsymbol\Sigma\)` such that `\(\mathrm{rank}(\boldsymbol\Sigma)= q\)`:

`$$\begin{array}{rcl}
  \text{latent space } &amp;   \mathbf{Z}_i \sim \mathcal{N}(\mathbf{o}_i + \mathbf{x}_i^\top\boldsymbol\Theta,\boldsymbol\Sigma), &amp; \boldsymbol\Sigma = \mathbf{B}\mathbf{B}^\top, \quad \mathbf{B}\in\mathcal{M}_{pq} \\
  \text{observation space } &amp;  Y_{ij} | Z_{ij} \quad \text{indep.} &amp; Y_{ij} | Z_{ij} \sim \mathcal{P}\left(\exp\{Z_{ij}\}\right),
\end{array}$$`

The dimension `\(q\)` of the latent space corresponds to the number of axes in the PCA or, in other words, to the rank of `\(\boldsymbol\Sigma = \mathbf{B}\mathbf{B}^\intercal\)`.


The unkwown parameters are  `\(\boldsymbol\Theta\)` and `\(\mathbf{B}\)`, the matrix of .important[_rescaled loadings_]

### Features

  - **Optimization**: _"Similar"_ variational framework (with different gradients)
  - **Model selection**: variational BIC/ICL
      - `\(\tilde{\text{BIC}}_q = J(\theta, q) - \frac12 \log(n) \left(p (d + q) - q(q-1)/2\right)\)`
      - `\(\tilde{\text{ICL}}_q = \tilde{\text{BIC}}_q - \mathcal{H}(q)\)`
  - **Vizualization:** PCA on the expected latent position `\(\mathbb{E}_{q}(\mathbf{Z}_i) -\mathbf{o}_i + \mathbf{x}_i^\top\boldsymbol\Theta  = \mathbf{M}\hat{\mathbf{B}}^\top\)`

---
# A PCA analysis of the oaks data set

Let us fit PLNPCA on our best model up to now (with TSS as offsets):


```r
PCA_offset &lt;- 
  PLNPCA(Abundance ~ 1 + offset(log(Offset)), data = oaks, 
         ranks = 1:30, control_main = list(cores = 10))
```


The ouput is of class (`PLNPCAfamily`). It a collection of `R` objects:


```r
PCA_offset
```

```
## --------------------------------------------------------
## COLLECTION OF 30 POISSON LOGNORMAL MODELS
## --------------------------------------------------------
##  Task: Principal Component Analysis
## ========================================================
##  - Ranks considered: from 1 to 30
##  - Best model (greater BIC): rank = 29
##  - Best model (greater ICL): rank = 29
```

`PLNPCAfamily` has three methods: `plot`, `getModel`, `getBestModel`&lt;sup&gt;1&lt;/sup&gt; 

.footnote[[1] Additional help can be found with `?PLNPCAfamily`, `?getBestModel.PLNPCAfamily`, `?plot.PLNPCAfamily`]

---
# PCA analysis: model selection (I)

The plot function gives you hints about the "right" rank/subspace size of your data


```r
plot(PCA_offset)
```

![](slides_files/figure-html/plot PLNPCA offset-1.png)&lt;!-- --&gt;

---
# PCA analysis: model selection (II)

To extract a particular model from the collection, use `getBestModel`:


```r
PCA_offset_BIC &lt;- getBestModel(PCA_offset, "BIC")
```


The extracted object has class `PLNPCAfit`. It inherits from the `PLNfit` class but with additional methods due to its `PCA` nature: when printing `PCA_offset_BIC`, we get


```
## Poisson Lognormal with rank constrained for PCA (rank = 29)
## ==================================================================
##  nb_param    loglik      BIC       ICL
##      3014 -45455.24 -52618.9 -49846.15
## ==================================================================
## * Useful fields
##     $model_par, $latent, $var_par, $optim_par
##     $loglik, $BIC, $ICL, $loglik_vec, $nb_param, $criteria
## * Useful S3 methods
##     print(), coef(), sigma(), vcov(), fitted(), predict(), standard_error()
## * Additional fields for PCA
##     $percent_var, $corr_circle, $scores, $rotation, $eig, $var, $ind
## * Additional S3 methods for PCA
##     plot.PLNPCAfit()
```

---
# PCA analysis: model exploration

Inheritance allows you to rely on the same methods as with `PLN`:

.pull-left[

```r
corrplot(
  cov2cor(sigma(M01_oaks)),
  tl.cex = .5)
```

![](slides_files/figure-html/PLN covariance M01-1.png)&lt;!-- --&gt;
]

.pull-right[

```r
corrplot(
  cov2cor(sigma(PCA_offset_BIC)),
  tl.cex = .5)
```

![](slides_files/figure-html/PLNPCA covariance-1.png)&lt;!-- --&gt;
]

---
# PCA: vizualisation 

&lt;small&gt;

```r
factoextra::fviz_pca_biplot(
  PCA_offset_BIC, select.var = list(contrib = 10), addEllipses = TRUE, habillage = oaks$tree,
  title = "Biplot (10 most contributing species)"
  ) + labs(col = "tree status") + scale_color_viridis_d()
```

&lt;img src="slides_files/figure-html/PCA offset vizu tree-1.png" style="display: block; margin: auto;" /&gt;
&lt;/small&gt;

---
# PCA: removing covariate effects

To hopefully find some hidden effects in the data, we can try to remove confounding ones:


```r
PCA_tree &lt;- PLNPCA(Abundance ~ 0 + tree + offset(log(Offset)), 
                   data = oaks, ranks = 1:30, control_main = list(cores = 10))
```

&lt;img src="slides_files/figure-html/PCA covariate tree plot-1.png" style="display: block; margin: auto;" /&gt;

---

class: center, middle, inverse

# Sparse structure estimation for counts  &lt;br/&gt; .small[See Chiquet, Mariadassou, and Robin [CMR19]]



---

# .small[Background on Gaussian Graphical Models]
  
Suppose `\(\mathbf{Z} \sim \mathcal{N}(\mathbf{0}, {\boldsymbol\Omega}^{-1}=\boldsymbol\Sigma)\)`
  
### Conditional independence structure

  `$$(i,j)  \notin  \mathcal{E}  \Leftrightarrow  Z_j  \perp  Z_k  | Z_{\backslash \{j,k\}} \Leftrightarrow {\boldsymbol\Omega}_{jk} = 0.$$`

.pull-left[
  `\(\mathcal{G}=(\mathcal{P},\mathcal{E})\)`![graph](slides_files/graph.png)
]

.pull-right[
  `\(\boldsymbol\Omega\)`![sparisty](slides_files/Markovadjacency.png)
  ]

### Graphical-Lasso

Network reconstruction is (roughly) a variable selection problem
`$$\hat{{\boldsymbol\Omega}}_\lambda=\arg\max_{\boldsymbol\Omega     \in     \mathbb{S}_+}
      \ell({\boldsymbol\Omega};\mathbf{Y})-\lambda \|{\boldsymbol\Omega}\|_{1}$$`

---

# Sparse precision for multivariate counts

The PLN-network model add a sparsity constraint on the precision matrix `\({\boldsymbol\Sigma}^{-1}\triangleq \boldsymbol\Omega\)`:

`$$\begin{array}{rcl}
  \text{latent space } &amp;   \mathbf{Z}_i \sim \mathcal{N}\left({\mathbf{o}_i + \mathbf{x}_i^\top\boldsymbol\Theta},\boldsymbol\Omega^{-1}\right) &amp;  \|\boldsymbol\Omega\|_1 &lt; c \\
    \text{observation space } &amp;  \mathbf{Y}_i | \mathbf{Z}_i \sim \mathcal{P}\left(\exp\{\mathbf{Z}_i\}\right)
  \end{array}$$`


`\(\rightsquigarrow\)` The `\(\ell_1\)`-penalty induces selection of direct relations (an underlying network)

## .small[Variational approximation]
  
`$$J(\theta, q)  - \lambda  \| \boldsymbol\Omega\|_{1,\text{off}} = \mathbb{E}_{q} [\log p_\theta(\mathbf{Y}, \mathbf{Z})] + \mathcal{H}[q(\mathbf{Z})] - \lambda  \|\boldsymbol\Omega\|_{1, \text{off}}$$`

Still bi-concave in `\((\boldsymbol\Omega, \boldsymbol\Theta)\)` and `\((\mathbf{M}, \mathbf{S})\)`.  

Solving in  `\(\boldsymbol\Omega\)` leads to

`$$\hat{\boldsymbol\Omega} = \arg\max_{\boldsymbol\Omega} \frac{n}{2} \left(\log | \boldsymbol\Omega | - \text{trace}(\hat{\boldsymbol\Sigma} \boldsymbol\Omega)\right) - \lambda \|\boldsymbol\Omega\|_{1, \text{off}}: \quad \text{graphical-Lasso problem}$$`
with `\(\hat{\boldsymbol\Sigma} = n^{-1}(\mathbf{M}^\top \mathbf{M} + \mathrm{diag}(\bar{\mathbf{S}}^2)\)`.

---
# Network inference on the oaks data set

- Infer a collections of networks indexed by the sparsity accounting for the tree effect.

`$$\text{EBIC}_\gamma(\hat{\boldsymbol\Omega}_\lambda)  =   -2 \textrm{loglik} (\hat{\boldsymbol\Omega}) + \log(n) (|\mathcal{E}_\lambda| + p d) + \gamma \log {p(p+1)/2 \choose |\mathcal{E}_\lambda|},$$`
       
.pull-left[
where `\(\mathcal{E}_\lambda\)` is the number of selected edge at `\(\lambda\)`.

&lt;br /&gt;

```r
networks_oaks_tree &lt;- 
  PLNnetwork(
    Abundance ~ 0 + tree + 
           offset(log(Offset)),
    data = oaks
  )
```

]

.pull-right[
![](slides_files/figure-html/plot network family site-1.png)&lt;!-- --&gt;
]

---
# PLNnetwork: field access

Let us plot the estimated correlation matrix, after regularization of its inverse, and the corresponding network of partial correlation.

.pull-left[
![](slides_files/figure-html/plot network site-1.png)&lt;!-- --&gt;
]

.pull-right[
![](slides_files/figure-html/plot net site-1.png)&lt;!-- --&gt;
]

---
# PLNnetwork: stability selection

An alternative to model selection criteria is the stability selection  - or StARS in the context of network.

1. Infers `\(B\)` networks `\({\boldsymbol\Omega}^{(b, \lambda)}\)` on subsamples of size `\(m\)` for varying `\(\lambda\)`.

2. Frequency of inclusion of each edges `\(e = i\sim j\)` is estimated by `$$p_e^\lambda = \# \{b: \Omega^{(b, \lambda)}_{ij} \neq 0\}/B$$`

3. Variance of inclusion of edge `\(e\)` is `\(v_e^\lambda = p_e^\lambda (1 - p_e^\lambda)\)`. 

4. Network stability is `\(\textrm{stab}(\lambda) = 1 - 2\bar{v}^\lambda\)` where `\(\bar{v}^\lambda\)` is the average of the `\(v_e^\lambda\)`. 

&lt;br /&gt;

`\(\rightsquigarrow\)` StARS&lt;sup&gt;1&lt;/sup&gt;  selects the smallest `\(\lambda\)` (densest network) for which `\(\textrm{stab}(\lambda) \geq 1 - 2\beta\)`

.footnote[Liu, Roeder, and Wasserman [LRW10] suggest using `\(2\beta = 0.05\)` and `\(m = \lfloor 10 \sqrt{n}\rfloor\)` based on theoretical results.]

---
# StARS

In `getBestModel`, when "StARS" is requested, stabilitiy selection is performed if needed:


```r
stability_selection(networks_oaks_tree, mc.cores = 10)
```




.pull-left[
![](slides_files/figure-html/plot network stars-1.png)&lt;!-- --&gt;
]

.pull-right[
![](slides_files/figure-html/plot net stars-1.png)&lt;!-- --&gt;
]


---
# Conclusion

## Summary

  - PLN = generic model for multivariate count data analysis
  - Flexible modeling of the covariance structure, allows for covariates
  - Efficient V-EM algorithm

## Extensions

- Other variants
      - zero inflation (data with _a lot_ of zeros)
      - covariance structures (spatial, time series, ...)
      - Variable selection ($\ell_1$-penalty on the regression coefficients)
- Other optimization approaches
     - large scale problem: ELBO + SG variant/GPU
     - Composite likelihood, starting from the variational solution
     - .important[log-likelihood] (MCMC, Stochastic gradient)
- Pave the way for Confidence interval and tests for regular PLN


---
# References

Aitchison, J. and C. Ho (1989). "The multivariate Poisson-log normal
distribution". In: _Biometrika_ 76.4, pp. 643-653.

Chiquet, J., M. Mariadassou, and S. Robin (2018). "Variational
inference for probabilistic Poisson PCA". In: _The Annals of Applied
Statistics_ 12, pp. 2674-2698. URL:
[http://dx.doi.org/10.1214/18-AOAS1177](http://dx.doi.org/10.1214/18-AOAS1177).

Chiquet, J., M. Mariadassou, and S. Robin (2019). "Variational
inference for sparse network reconstruction from count data". In:
_Proceedings of the 19th International Conference on Machine Learning
(ICML 2019)_.

Inouye, D. I., E. Yang, G. I. Allen, et al. (2017). "A review of
multivariate distributions for count data derived from the Poisson
distribution". In: _Wiley Interdisciplinary Reviews: Computational
Statistics_ 9.3.

Jakuschkin, B., V. Fievet, L. Schwaller, et al. (2016). "Deciphering
the pathobiome: intra-and interkingdom interactions involving the
pathogen Erysiphe alphitoides". In: _Microbial ecology_ 72.4, pp.
870-880.

Johnson, S. G. (2011). _The NLopt nonlinear-optimization package_. URL:
[http://ab-initio.mit.edu/nlopt](http://ab-initio.mit.edu/nlopt).

Karlis, D. (2005). "EM algorithm for mixed Poisson and other discrete
distributions". In: _Astin bulletin_ 35.01, pp. 3-24.

Liu, H., K. Roeder, and L. Wasserman (2010). "Stability Approach to
Regularization Selection (StARS) for High Dimensional Graphical
Models". In: _Proceedings of the 23rd International Conference on
Neural Information Processing Systems - Volume 2_. USA, pp. 1432-1440.

Mardia, K., J. Kent, and J. Bibby (1979). _Multivariate analysis_.
Academic press.

Svanberg, K. (2002). "A class of globally convergent optimization
methods based on conservative convex separable approximations". In:
_SIAM journal on optimization_ 12.2, pp. 555-573.

Wainwright, M. J. and M. I. Jordan (2008). "Graphical Models,
Exponential Families, and Variational Inference". In: _Found. Trends
Mach. Learn._ 1.1-2, pp. 1-305.

Westling, T. and T. H. McCormick (2015). _Beyond prediction: A
framework for inference with variational approximations in mixture
models_. arXiv: 1510.08151 [stat.ME].
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
